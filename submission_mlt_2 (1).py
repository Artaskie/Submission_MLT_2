# -*- coding: utf-8 -*-
"""Submission_MLT_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RY-C1QFZmqWNNqg1J5Gr_DR0tADKQiQH

# Proyek Sistem Rekomendasi Film:
- **Nama:** [Achmad Faiz Izzi]
- **Email:** [ahmadfaiz8365@gmail.com]
- **ID Dicoding:** [MC227D5Y1450]

# **Import Library**
"""

import os, shutil
# Manipulasi data
import pandas as pd
import numpy as np
# Visualisasi (opsional jika ingin eksplorasi data)
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from pathlib import Path
# Content-Based Filtering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Testing
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""**Insight:**

Beragam library digunakan untuk mendukung proses analisis data dan pengembangan model. Pandas dimanfaatkan untuk pengolahan data dalam format tabel, sedangkan NumPy digunakan dalam melakukan perhitungan numerik. Untuk keperluan visualisasi, digunakan Seaborn dan Matplotlib. TensorFlow berperan dalam membangun dan melatih model rekomendasi berbasis machine learning. Sementara itu, scikit-learn digunakan dalam proses ekstraksi fitur teks menggunakan TF-IDF, penerapan algoritma berbasis kedekatan data, serta membagi dataset menjadi data latih dan data validasi.

# **Data Understanding**

Pada tahap ini, dua file dataset yaitu `movies.csv` dan `ratings.csv` dimuat ke dalam DataFrame menggunakan library pandas. File `movies.csv` berisi informasi mengenai film seperti ID, judul, dan genre, sedangkan `ratings.csv` mencatat interaksi pengguna dengan film berupa rating yang diberikan. Setelah kedua file berhasil dimuat, keduanya digabungkan berdasarkan kolom movieId untuk membentuk satu DataFrame utama yang menyatukan data rating dengan informasi film terkait. Langkah ini penting agar setiap interaksi pengguna tidak hanya berisi angka penilaian, tetapi juga dilengkapi dengan konteks seperti judul dan genre film, sehingga dapat mendukung proses analisis serta pembangunan sistem rekomendasi yang lebih informatif dan akurat.
"""

from google.colab import files
files.upload()

# Buat direktori jika belum ada
os.makedirs("/root/.kaggle", exist_ok=True)
# Pindahkan file ke direktori .kaggle
shutil.move("kaggle (4).json", "/root/.kaggle/kaggle.json")
# Atur permission agar tidak terlalu terbuka
os.chmod("/root/.kaggle/kaggle.json", 600)

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d parasharmanas/movie-recommendation-system
!unzip movie-recommendation-system.zip

"""**Loading Dataset**"""

movies = pd.read_csv("movies.csv")
movies.head()

ratings = pd.read_csv("ratings.csv")
ratings.head()

df = pd.merge(movies, ratings, on='movieId')
df.drop(columns=['timestamp'], inplace=True)
df

"""Output yang ditampilkan di atas merupakan isi dari dua dataset yang dugunakan dalam sistem rekomendasi ini

1. Dataset "Movies"
Berisi informasi dasar tentang film:
- movieId: ID unik untuk setiap film.
- title: Judul film beserta tahun rilis.
- genres: Genre film yang dipisahkan oleh tanda "|".

2. Dataset "Ratings"
Berisi data penilaian pengguna terhadap film:
- userId: ID pengguna yang memberikan rating.
- movieId: ID film yang diberi rating.
- rating: Skor atau penilaian film dari pengguna (biasanya dari 0.5 sampai 5).
- timestamp: Waktu penilaian dalam format UNIX timestamp.

# **Exploratory Data Analysis**

**Deskripsi Variabel**
"""

df.info()

"""Dataset gabungan **Movies** dan **Ratings** berisi 25.000.095 entri dengan lima kolom: `movieId` (ID film), `title` (judul film), `genres` (genre film), `userId` (ID User), dan `rating` (rating film). Semuanya lengkap tanpa data kosong, dan menggunakan memori sekitar 953.7 MB."""

df.describe()

"""Statistik rating menunjukkan bahwa dari total 25 juta data, **rata-rata rating** film adalah **3.53** dengan **standar deviasi** sebesar **1.06**, yang menandakan variasi penilaian pengguna. Nilai rating minimum adalah **0.5** dan maksimum **5.0**. Sebagian besar rating berada di antara **3.0 (kuartil 1)** dan **4.0 (kuartil 3)**, dengan **nilai tengah (median)** di **3.5**, menunjukkan bahwa pengguna cenderung memberikan rating positif"""

# Jumlah rating per film (top 10)
df_counts = df['movieId'].value_counts()
print("\nJumlah rating per film (top 10):")
print(df_counts.head(10))

"""Data ini menunjukkan **10 film teratas dengan jumlah rating terbanyak**. Film dengan `movieId 356` menerima rating paling banyak, yaitu **81.491 rating**, diikuti oleh `movieId 318` dengan **81.482 rating**, dan seterusnya. Ini menunjukkan bahwa film-film tersebut sangat populer atau sering ditonton oleh pengguna dalam dataset."""

# Visualisasi distribusi rating
sns.boxplot(x=df['rating'])
plt.title('Distribusi Rating')
plt.show()

"""Box plot menggambarkan bahwa sebagian besar rating berkisar antara 3 hingga 4, menunjukkan kecenderungan pengguna untuk memberikan penilaian positif terhadap film yang mereka saksikan."""

rating_counts = df.groupby('movieId')['rating'].count().sort_values(ascending=False)
rating_counts.head(20).plot(kind='bar', figsize=(10,5))
plt.title('Top 20 Film dengan Rating Terbanyak')
plt.xlabel('Movie ID')
plt.ylabel('Jumlah Rating')
plt.show()

"""Grafik batang ini menampilkan 20 film yang menerima jumlah rating terbanyak dari pengguna. Informasi ini berguna untuk mengidentifikasi film-film populer yang berpotensi menjadi prioritas dalam sistem rekomendasi berbasis popularitas.

**Analisis Univariat**
"""

print('Banyak data id film : ', len(df.movieId.unique()))
print('Banyak data judul film : ', len(df.title.unique()))
print('Banyak data genre film : ', len(df.genres.unique()))
print('Banyak data id pengguna : ', len(df.userId.unique()))

print(df['rating'].value_counts().sort_index())

genre_set = set()
for genre_string in movies['genres'].dropna():
    genres = genre_string.split('|')
    genre_set.update(genres)

print("Jumlah genre unik:", len(genre_set))
print("Daftar genre unik:")
for genre in sorted(genre_set):
    print("-", genre)

"""**Insight:**

Dataset ini mencakup 162.541 pengguna unik dan 59.047 film unik dengan total 58.958 judul film yang berbeda, menunjukkan adanya sejumlah kecil duplikasi atau variasi dalam penulisan judul. Nilai rating tersebar dari 0.5 hingga 5.0 dengan interval 0.5, mencerminkan sistem penilaian berbasis bintang setengah. Genre film mencakup 20 kategori unik, mulai dari aksi, drama, hingga film noir dan dokumenter, serta termasuk label "(no genres listed)" yang menunjukkan adanya data tidak lengkap.

# **Data Preparation**
"""

# Mendeteksi dan menangani missing values
df.isna().sum()

"""Output menunjukkan bahwa tidak ada missing value pada dataset, sehingga data dinyatakan lengkap dan siap diproses lebih lanjut tanpa perlu penanganan khusus seperti pembersihan data."""

# Mendeteksi dan menangani data duplikat
print(df.duplicated().sum())

"""Output menunjukkan bahwa tidak ada data duplikat pada dataset, sehingga data dinyatakan lengkap dan siap diproses lebih lanjut

**Mensample dataset**

Tahap ini dilakukan untuk mengurangi ukuran dataset menjadi subset yang lebih kecil (500.000 baris) agar proses eksplorasi data, pelatihan model, dan evaluasi dapat dilakukan dengan lebih cepat dan efisien Penggunaan `random_state=42` memastikan bahwa proses sampling bersifat reproducible, sehingga hasil eksperimen tetap konsisten setiap kali kode dijalankan.
"""

df_sample = df.sample(500000, random_state=42).copy()

"""**Preprocesssing Content-Based Filtering**

Tahap preprocessing untuk Content-Based Filtering diperlukan agar fitur teks seperti genre dapat diubah menjadi format numerik yang bisa diproses oleh algoritma, salah satunya menggunakan TF-IDF (Term Frequency-Inverse Document Frequency).
"""

# Mengubah genre film menjadi representasi vektor TF-IDF
movie_features = df_sample.drop_duplicates('movieId')[['movieId', 'title', 'genres']].reset_index(drop=True)

tfidf = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b")
tfidf_matrix = tfidf.fit_transform(movie_features['genres'])

cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cbf_df = df_sample.copy()

"""**Preprocesssing Content-Based Filtering**

Preprocessing untuk Collaborative Filtering bertujuan untuk menyiapkan data rating agar sesuai dengan format yang dibutuhkan oleh library Surprise, sehingga proses pemodelan dapat berjalan dengan lancar dan akurat.
"""

user_ids = cbf_df['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

movie_ids = cbf_df['movieId'].unique().tolist()
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

cbf_df['user'] = cbf_df['userId'].map(user_to_user_encoded)
cbf_df['movie'] = cbf_df['movieId'].map(movie_to_movie_encoded)

num_users = len(user_to_user_encoded)
num_movies = len(movie_encoded_to_movie)
min_rating = cbf_df['rating'].min()
max_rating = cbf_df['rating'].max()

print(f'Users: {num_users}, Movies: {num_movies}, Rating range: {min_rating}â€“{max_rating}')

"""Output tersebut menandakan bahwa data rating sudah berada dalam rentang valid (0.5 sampai 5.0), terdapat 118.241 pengguna, dan terdapat 18.214 film.

**Splitting Data**
"""

# Membagi Data untuk Training dan Validasi
x = cbf_df[['user', 'movie']].values
y = cbf_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""Membagi data rating menjadi data latih dan data validasi dengan rasio 80:20 dan menggunakan random_state untuk memastikan pembagian data yang konsisten.

# **Model Development dengan Content-Based Filtering**
"""

def rekomendasi_dengan_genre(genre_input, top_n=5):
    genre_mask = movie_features['genres'].str.contains(genre_input, case=False, na=False)
    matching_movies = movie_features[genre_mask]

    if matching_movies.empty:
        print(f"Tidak ada film pada genre '{genre_input}' ditemukan.")
        return

    idx = matching_movies.index[0]
    sim_scores = list(enumerate(cos_sim[idx]))

    sim_scores = [x for x in sim_scores if x[0] != idx]

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_n]

    print(f"Rekomendasi film mirip berdasarkan genre '{genre_input}' dengan referensi:")
    print(f"> {movie_features.iloc[idx]['title']} | Genre: {movie_features.iloc[idx]['genres']}")

    for i, (movie_idx, _) in enumerate(sim_scores, 1):
        title = movie_features.iloc[movie_idx]['title']
        genres = movie_features.iloc[movie_idx]['genres']
        print(f"{i}. {title} | Genre: {genres}")

# Uji coba rekomendasi film dengan input Genre
rekomendasi_dengan_genre("Drama")

def rekomendasi_film_content(title, top_n=5):
    match = movie_features[movie_features['title'].str.lower() == title.lower()]
    if match.empty:
        print("Judul film tidak ditemukan.")
        return

    idx = match.index[0]
    sim_scores = list(enumerate(cos_sim[idx]))

    sim_scores = [x for x in sim_scores if x[0] != idx]

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_n]

    print(f"Rekomendasi untuk: {movie_features.iloc[idx]['title']} | Genre: {movie_features.iloc[idx]['genres']}")
    for i, (movie_idx, _) in enumerate(sim_scores, 1):
        rec_title = movie_features.iloc[movie_idx]['title']
        rec_genre = movie_features.iloc[movie_idx]['genres']
        print(f"{i}. {rec_title} | Genre: {rec_genre}")

# Uji coba rekomendasi film dengan input Judul
rekomendasi_film_content("Waiting to Exhale (1995)")

"""**Insight:**

Model Content-Based Filtering yang dikembangkan memberikan rekomendasi film berdasarkan kemiripan konten, khususnya dari informasi genre. Dengan memanfaatkan teknik TF-IDF untuk mengekstraksi fitur dari kolom genre dan menghitung kemiripan antar film menggunakan cosine similarity, sistem mampu menyarankan film yang memiliki karakteristik serupa dengan film tertentu atau genre yang dipilih pengguna. Pendekatan ini tidak memerlukan data historis dari pengguna, sehingga efektif untuk mengatasi permasalahan cold-start pada pengguna baru. Rekomendasi yang dihasilkan bersifat personal dan dapat dijelaskan secara transparan, karena didasarkan pada kesamaan fitur konten film.

# **Model Development dengan Collaborative Filtering**
"""

def rekomendasi_film_dari_user(user_id, model, cbf_df, user_to_user_encoded, user_encoded_to_user,
                              movie_to_movie_encoded, movie_encoded_to_movie, top_k=10):

    if user_id not in user_to_user_encoded:
        print(f"User {user_id} tidak ditemukan di data training.")
        return []

    user_encoded = user_to_user_encoded[user_id]

    movies_watched = cbf_df[cbf_df['userId'] == user_id]['movieId'].unique()
    movies_not_watched = [m for m in movie_to_movie_encoded.keys() if m not in movies_watched]
    movies_not_watched_encoded = [movie_to_movie_encoded[m] for m in movies_not_watched]

    user_array = np.array([user_encoded] * len(movies_not_watched_encoded))
    movie_array = np.array(movies_not_watched_encoded)
    input_array = np.vstack((user_array, movie_array)).T

    pred_ratings = model.predict(input_array, verbose=0).flatten()

    top_indices = pred_ratings.argsort()[-top_k:][::-1]
    top_movie_encoded = [movies_not_watched_encoded[i] for i in top_indices]
    top_movie_ids = [movie_encoded_to_movie[m] for m in top_movie_encoded]

    print(f"\nRekomendasi {top_k} film untuk user {user_id}:")

    for movie_id in top_movie_ids:
        title = cbf_df[cbf_df['movieId'] == movie_id]['title'].values
        if len(title) > 0:
            print(f"- {title[0]}")
        else:
            print(f"- Movie ID {movie_id} (judul tidak ditemukan)")

    return top_movie_ids

"""**Insight:**

Model Collaborative Filtering yang digunakan bertujuan memberikan rekomendasi film yang dipersonalisasi berdasarkan pola interaksi pengguna. Dengan memanfaatkan model neural network yang telah dilatih, sistem ini memprediksi seberapa besar kemungkinan seorang pengguna akan menyukai film yang belum pernah ditonton, berdasarkan pola rating yang diberikan sebelumnya. Setiap pengguna dan film direpresentasikan dalam bentuk vektor embedding, dan prediksi rating dihitung dari kombinasi vektor tersebut. Model ini efektif dalam menangkap preferensi tersembunyi pengguna dan memberikan rekomendasi yang relevan, terutama ketika riwayat interaksi cukup tersedia. Namun, pendekatan ini bergantung pada data historis dan kurang efektif untuk pengguna baru yang belum memiliki interaksi (cold-start problem).

**Proses Training**
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.movie_embedding = layers.Embedding(
            num_movies, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])

        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)
        x = dot_user_movie + user_bias + movie_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_movies, embedding_size=50)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x_train, y_train,
    batch_size=256,
    epochs=3,
    validation_data=(x_val, y_val)
)

# Uji coba pemberian rekomendasi
user_sample = cbf_df['userId'].sample(1).iloc[0]
rekomendasi_film_dari_user(
    user_sample,
    model,
    cbf_df,
    user_to_user_encoded,
    user_encoded_to_user,
    movie_to_movie_encoded,
    movie_encoded_to_movie,
    top_k=10
)

"""**Insight:**

Proses training pada model Collaborative Filtering ini dilakukan dengan pendekatan neural network menggunakan arsitektur embedding untuk memetakan pengguna dan film ke dalam ruang representasi laten. Model dilatih dengan pasangan data pengguna dan film (`x_train`) serta nilai rating yang telah dinormalisasi (`y_train`). Selama pelatihan, model mempelajari hubungan antara pengguna dan film melalui dot product embedding, ditambah bias, lalu diaktifkan dengan fungsi sigmoid. Proses optimisasi dilakukan menggunakan algoritma Adam dengan learning rate 0.001, dan kinerjanya dievaluasi menggunakan metrik Root Mean Squared Error (RMSE). Selama tiga epoch pelatihan, model juga divalidasi menggunakan data terpisah untuk memastikan performa yang stabil dan menghindari overfitting. Insight utama dari proses ini adalah bahwa model dapat secara bertahap menyesuaikan bobotnya untuk memprediksi preferensi pengguna secara akurat berdasarkan pola historis.

# **Evaluation**
"""

# Visualisasi Metrix
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('RMSE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Plot di atas menunjukkan nilai Root Mean Squared Error (RMSE) pada data training dan testing selama proses training model selama 3 epoch.

- Epoch 0â€“1: RMSE pada training menurun (model membaik), tapi RMSE pada testing meningkat â€” ini bisa menjadi tanda awal overfitting.
- Epoch 1â€“2: RMSE pada training meningkat kembali, dan RMSE pada testing sedikit menurun, namun masih lebih tinggi dari awal.

**Evaluasi dengan RMSE**
"""

train_rmse_last = history.history['root_mean_squared_error'][-1]
val_rmse_last = history.history['val_root_mean_squared_error'][-1]
print(f"Final RMSE - Train: {train_rmse_last:.4f}, Validation: {val_rmse_last:.4f}")

"""Model menghasilkan nilai RMSE sebesar 0.2482 pada data pelatihan dan 0.2502 pada data validasi. Hasil ini menunjukkan model mampu memprediksi dengan akurat dan tidak menunjukkan tanda-tanda overfitting."""